{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Programming & Close Reading #\n",
    "\n",
    "The rationale of this project is found in what feels to me as a still uncomfortable clash between hermeneutics and distant reading methods. We understand and accept that quantitative approaches can tell us a lot about texts \\marginpar{ JZ_20160415_1652: Refs. to examples to be added. } At the same time well known practitioners of such methods tell us that in the end the patterns that emerge from number crunching and pattern recognition require hermeneutic interpretation \\marginpar{ JZ_20160415_1657: Refs. to Kirschenbaum, Meister, Ramsay, Underwood etc. } to be given meaning. I assert a strong dichotomic predilection in DH research to this matter. It seems necessarily to be one of two. Patterns can be modeled and quantified, but this necessarily results in reductive measures that are lossy of the subtle distinctions that drive hermeneutic interpretation. The gain of this coarse reductivenes is stringent formalization, which ensures computational tractability and therefor the scale and power of the analysis of large numbers: we can measure into corpora without even looking at them with human eyes and intellect. The other option is to apply subtle hermeneutics through close reading a text. This gives the research the power of meticulous interpretation, of precise contextualizing of meaning, of capturing, representing, and iterpretating complex heterogeneous knowledge. The loss here however is the power of scale: such hermeneutic precision can not be expressed by simple numbers, such heterogeneity cannot be modeled at scale. Therefor the hermeneutic model is a model of a single or a few texts and a model without computation, it is *only* interpretation. The quantitative model stands in opposition: this counts, and by using the computer it counts eerily fast and into huge corpora—but its understanding of the individual texts in corpora is poor. \n",
    "\n",
    "My conjecture is that the root cause for this perceived dichotomy is the preconception that software must scale—that the usefulness of writing and reading with software, thus the usefulness of code literacy is limited to tasks that are repetitive and thus subject to automation. However, what if we would not focus on scale for a change. What if we apply the values of close reading (attention, detailism .. .. ) to programming? What would formalizing thus—as Slow Programming—in code the process of close reading tells us about a text? This Notebook is a experimental quest towards an answer to that question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment ##\n",
    "\n",
    "I contend that hermeneutics and interpretation are not mutual exclusive with code. Softare and automization *can* be used as reductive methods that limit interpretation or are only crude hermeneutic means on the level of code, but i assert that they need not be. In fact, if anything, code in its form of literate programming \\marginpar{ JZ_20160416_1749: Ref. Knuth } is a meticulous precise description of process. Next to that code is also 'just text', just another semiotic system \\marginpar{ JZ_20160416_1751: Ref. Knuth/Sample/Marino? }. Therefore, if we agree that text is an excellent means for reporting hermeneutic process, code should even be better—because it is 'just' text, but with an edge: it will reproduce process meticulously, as long as we capture the hermeneutic process precisely enough. This is what I want to try to do in this experiment: model each and all hermeneutic choices when reading/editing a text meticulousely into code. I will use an Object Oriented approach \\marginpar{ JZ_20160416_1755:  Some Ref. } to devising the model and code. I will furthermore hold to these rules when 'Close Reading' a text using code:\n",
    "\n",
    "1 Only direct and indirect speech may be represented as string instances.\n",
    "2 There will be no ‘ghost’ objects or methods (unused program code) during the execution of the program.\n",
    "3 The resulting program should execute without producing Ruby exceptions or runtime errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## First, let's do no harm ##\n",
    "\n",
    "Right after laying down those rules I realized that if I want to hold myself to the meticulous precise registration of process I should not transform the 'raw' text by using a text processor (that is: changing the representation by typing and editing the text file), because such requires interpretation and all interpretative acts should be modeled or captured in code. So, I can't change the source file I will be using. Two observations now arise…\n",
    "\n",
    "The firs is that this file in itself is the result of downloading an edition of the text of the Middledutch Reynaert, and OCR'ing using tesseract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.2.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
